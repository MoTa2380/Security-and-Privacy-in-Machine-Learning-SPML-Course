{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar1V8Q3ASeqy"
      },
      "source": [
        "## Student Number: 99101321\n",
        "\n",
        "## Name: Mohammad Taslimi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRrvAChBVXEf"
      },
      "source": [
        "# **Notes**\n",
        "In this notebook you are going to implement [NES](https://arxiv.org/pdf/1804.08598.pdf) blackbox attack and test in on Cifar10. First, you must implement the provided functions. Then you must test the attack and report the average number of queries and the success rate of the attack. You can define as many additional functions as you want. If you want to alter the structure of the provided code, please do as minimally as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bUw1fnES7vn"
      },
      "source": [
        "# **Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovEZsWD5K_Ey"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from typing import Type\n",
        "import numpy as np\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8mt4gp2S-c6"
      },
      "source": [
        "#**Defining the Model and Dataloader**\n",
        "Here the link to a ResNet18 checkpoint is provided. Please use this checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqMGQOsaK9Sf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ec481b-9d03-4905-8eae-1e5583512b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [02:58<00:00, 953482.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/cifar10/cifar-10-python.tar.gz to /content/cifar10/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(),])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='/content/cifar10/', train = True, download = True, transform = transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='/content/cifar10/', train = False, download = True, transform = transforms.Compose([transforms.ToTensor(),]))\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oKXfOvsB7Dw"
      },
      "outputs": [],
      "source": [
        "class ResNet18(nn.Module):\n",
        "  def __init__(self, num_cls):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        *list(resnet18(weights=None).children())[:-2])\n",
        "\n",
        "    self.fc = nn.Linear(512, num_cls)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "    logits = self.fc(x)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkfqulEKPraZ",
        "outputId": "e479e2e7-cc0b-4414-d0b0-57d609b78762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-k2y76KAtvFxXDVPMDIzXtVa0sDgF0MQ\n",
            "From (redirected): https://drive.google.com/uc?id=1-k2y76KAtvFxXDVPMDIzXtVa0sDgF0MQ&confirm=t&uuid=707a5a65-a169-463b-a5ee-0212558fa8c9\n",
            "To: /content/resnet18_cifar10_model.pt\n",
            "100% 44.8M/44.8M [00:01<00:00, 29.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1-k2y76KAtvFxXDVPMDIzXtVa0sDgF0MQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q1VAHu1BwM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a9c444a-020a-4ef9-e2ca-008ca4ff4fcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test set: 83.87\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "CIFAR10_model_PATH = \"/content/resnet18_cifar10_model.pt\"\n",
        "state_dict = torch.load(CIFAR10_model_PATH)\n",
        "\n",
        "\n",
        "model = ResNet18(num_cls=10).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the model on the test set: {100 * correct / total}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isdk2N3kZE4k"
      },
      "source": [
        "# **Natural Evolutionary Strategies (NES)**\n",
        "\n",
        "complete the functions in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOtCzRz9VS8i"
      },
      "outputs": [],
      "source": [
        "def estimate_grad(model, images, labels, search_variance, n):\n",
        "    \"\"\"\n",
        "    NES Gradient Estimate\n",
        "\n",
        "    inputs:\n",
        "            - model: the target model (only used for computing the loss)\n",
        "            - images: Tensor containing images. size: [number of images, 3, image_dim, image_dim]\n",
        "            - labels: Tensor containing the original labels of images.\n",
        "            - search_variance: sigma\n",
        "            - n: number of samples to estimate the gradient\n",
        "\n",
        "    outputs:\n",
        "            - g: estimated gradients. has similar shape as images.\n",
        "\n",
        "    Guide:  - Define g with initial value 0.\n",
        "            - for n iterations do:\n",
        "                - Define a tensor of random Gaussian noise with the input image shape (use torch.randn)\n",
        "                - Divide this tensor by (sqrt(image_dim*image_dim*3)). (we do this due to the properties of gaussian distribution in high-dimensional space.)\n",
        "                - Compute the gradient of the loss with finite difference method using the defined noise tensor.\n",
        "                - Add the comupted value to g\n",
        "    \"\"\"\n",
        "\n",
        "    ###################################\n",
        "    #Your Code Goes Here (15 pt.)\n",
        "\n",
        "    g = torch.zeros_like(images)\n",
        "\n",
        "    _, _, image_dim, _ = images.shape\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for _ in range(n):\n",
        "          noise = torch.randn_like(images) / torch.sqrt(torch.tensor(image_dim * image_dim * 3.0))\n",
        "\n",
        "          perturbed_images_plus = images + search_variance * noise\n",
        "          perturbed_images_minus = images - search_variance * noise\n",
        "\n",
        "          output_plus = model(perturbed_images_plus)\n",
        "          output_minus = model(perturbed_images_minus)\n",
        "\n",
        "          loss_plus = torch.nn.functional.cross_entropy(output_plus, labels)\n",
        "          loss_minus = torch.nn.functional.cross_entropy(output_minus, labels)\n",
        "\n",
        "          gradient = ((loss_plus - loss_minus) * (noise)) / (search_variance)\n",
        "\n",
        "          g += gradient\n",
        "\n",
        "    return g\n",
        "\n",
        "\n",
        "\n",
        "def one_iteration_pgd_attack(grad, images, original_images, args):\n",
        "     perturbation = args['epsilon'] * grad.sign()\n",
        "     perturbed_images = images + perturbation\n",
        "     perturbed_images = torch.clamp(perturbed_images, original_images - args['delta'], original_images + args['delta'])\n",
        "     perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
        "\n",
        "     return perturbed_images\n",
        "\n",
        "def generate_attacks(model, images, labels, args):\n",
        "    '''\n",
        "    The process for generating blackbox adversarial examples. Implement for l_infty attack.\n",
        "\n",
        "    inputs:\n",
        "            - model: The target model\n",
        "            - images: Tensor containing images of a batch. size: [batch_size, 3, image_dim, image_dim]\n",
        "            - labels: Tensor containing the original labels of images. size: [batch_size, num_classes]\n",
        "\n",
        "    outputs:\n",
        "            - attacks: Must have the same shape as images.\n",
        "            - total_queries: number of queries till a successful attack for each sample\n",
        "            - success: Flag showing if each attack was successful or not.\n",
        "    '''\n",
        "\n",
        "    batch_size = images.shape[0]\n",
        "    total_queries = torch.zeros(batch_size)\n",
        "    success = torch.zeros(batch_size)\n",
        "    attacks = images.clone()\n",
        "    attacks = attacks.to(device)\n",
        "    success = success.to(device)\n",
        "    total_queries = total_queries.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      while not torch.any(total_queries > args[\"max_queries\"]):\n",
        "          unsuccessful_indices = (success == 0).nonzero(as_tuple=True)[0]\n",
        "          unsuccessful_indices = unsuccessful_indices.to(device)\n",
        "          if len(unsuccessful_indices) == 0:\n",
        "              break\n",
        "          unsuccessful_images = attacks[unsuccessful_indices]\n",
        "          unsuccessful_labels = labels[unsuccessful_indices]\n",
        "\n",
        "          grad = estimate_grad(model, unsuccessful_images, unsuccessful_labels, args['search_variance'], args['gradient_num_samples'])\n",
        "\n",
        "          unsuccessful_attacks = one_iteration_pgd_attack(grad, unsuccessful_images, images[unsuccessful_indices], args)\n",
        "\n",
        "          attacks[unsuccessful_indices] = unsuccessful_attacks\n",
        "\n",
        "          outputs = model(attacks)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "          successful_attacks = (predicted != labels).float()\n",
        "          success = torch.max(success, successful_attacks)\n",
        "\n",
        "          total_queries[unsuccessful_indices] += args['gradient_num_samples']*2\n",
        "\n",
        "    return attacks, total_queries, success\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWdQXoR3YS58"
      },
      "source": [
        "# **Attack Report**\n",
        "\n",
        "Report the success rate and average number of qureies for blackbox $l_∞$ PGD attacks for $\\sigma \\in \\{0.001, 0.01\\}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr72hNYhXO7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6b97ee-6abf-407e-ab7a-256024ef51b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Sigma: 0.01\n",
            "Success Rate: 54.95%\n",
            "Average Number of Queries: 611.53\n",
            "Sigma: 0.001\n",
            "Success Rate: 55.50%\n",
            "Average Number of Queries: 610.12\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the arguments\n",
        "args = {'max_queries': 1000, # maximum number of queries for one sample\n",
        "        'search_variance': 0.01,  # sigma (will be modified for each experiment)\n",
        "        'epsilon': 0.01,    # step size in pgd\n",
        "        'delta': 0.05,      # radius on which the attack must be projected\n",
        "        'batch_size': 10,\n",
        "        'gradient_num_samples': 15, # number of samples used for estimating the gradients\n",
        "        }\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./cifar10/', train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "test_indices = list(range(len(testset)))\n",
        "_, subset_indices = train_test_split(test_indices, test_size=0.2, random_state=42)\n",
        "subset_testset = Subset(testset, subset_indices)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(subset_testset, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "\n",
        "def run_attack_for_sigma(sigma):\n",
        "    args['search_variance'] = sigma\n",
        "    total_success = 0\n",
        "    total_queries = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "      for images, labels in testloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          attacks, queries, success = generate_attacks(model, images, labels, args)\n",
        "          total_success += success.sum().item()\n",
        "          total_queries += queries.sum().item()\n",
        "          total_samples += len(labels)\n",
        "\n",
        "    success_rate = (total_success / total_samples) * 100\n",
        "    average_queries = total_queries / total_samples\n",
        "\n",
        "    return success_rate, average_queries\n",
        "\n",
        "sigmas = [0.01, 0.001]\n",
        "results = {}\n",
        "\n",
        "for sigma in sigmas:\n",
        "    success_rate, average_queries = run_attack_for_sigma(sigma)\n",
        "    results[sigma] = {'success_rate': success_rate, 'average_queries': average_queries}\n",
        "\n",
        "\n",
        "for sigma, result in results.items():\n",
        "    print(f\"Sigma: {sigma}\")\n",
        "    print(f\"Success Rate: {result['success_rate']:.2f}%\")\n",
        "    print(f\"Average Number of Queries: {result['average_queries']:.2f}\")\n",
        "\n",
        "####################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "s9jO1WQPac-P"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}