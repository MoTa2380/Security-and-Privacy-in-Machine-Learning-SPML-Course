{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooJtNr5dGrH9"
      },
      "source": [
        "**Name: Mohammad Taslimi**\n",
        "\n",
        "**Student Number: 99101321**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJm9Z1k0cdmh"
      },
      "source": [
        "# Neural-Network with Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDN075MYGesD"
      },
      "source": [
        "In this notebook, you are going to write and implement all the components required to create and train a two-layered neural network using NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt3FdxgNcdmm"
      },
      "source": [
        "## Imports & Seeding:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPZ4zlnxqhl5"
      },
      "source": [
        "Importing some common libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Et7OS7TGcdmn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(123)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa2v2-xbcdmo"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKWqV2Gycdmp"
      },
      "source": [
        "You'll train and evaluate your model on [Fashion MNIST](https://en.wikipedia.org/wiki/Fashion_MNIST) dataset. In this section, you'll download Fashion MNIST and split it into training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tMYZtSoLc7c-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6728d4bf-d0df-4796-8000-0bfd9d8867f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70000, 784) (70000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Using `fetch_openml`, download `Fashion-MNIST`\n",
        "# and save the training data and labels in `X` and `y` respectively.\n",
        "#############################\n",
        "# Your code goes here (5 points)\n",
        "X, y = fetch_openml('Fashion-MNIST', version=1, parser='auto', return_X_y=True)\n",
        "#############################\n",
        "\n",
        "# Normalization:\n",
        "X = ((X / 255.) - .5) * 2\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sDmxyMJ4dBk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0013a61f-702e-49cb-fdd5-9f451f80acd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784) (60000,) (10000, 784) (10000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Using `train_test_split`, split your data into two sets.\n",
        "# Set the test_size to 10000\n",
        "\n",
        "#############################\n",
        "# Your code goes here (6 points)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=123)\n",
        "#############################\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiGTXGXKcdmt"
      },
      "source": [
        "## Prepare training & validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3nNYlDcdmt"
      },
      "source": [
        "We'll use only 3 classes from Fashion MNIST: Trouser, T-shirt, and Sneaker classes.\n",
        "\n",
        "The class labels for T-shirt, Trouser, and Sneaker are 0, 1, and 7 respectively.\n",
        "\n",
        "In this part, you'll limit the testing and training sets to only these three classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TcBDZEtzcdmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2dfff8-0068-49ef-a46f-eddac72497e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18017, 784) (18017,)\n"
          ]
        }
      ],
      "source": [
        "# Modify `y_train` and `x_train`.\n",
        "# Only keep the 3 classes mentioned above.\n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "\n",
        "\n",
        "\n",
        "# Define the classes we are interested in\n",
        "classes = ['0', '1', '7']\n",
        "\n",
        "# Convert y_train and y_test to numpy arrays for easier processing\n",
        "y_train = y_train.to_numpy()\n",
        "\n",
        "# Get the indices of the samples belonging to the above classes in the training set\n",
        "indices_train = np.where(np.isin(y_train, classes))[0]\n",
        "\n",
        "# Keep only the samples of the above classes in the training set\n",
        "x_train = x_train.iloc[indices_train]\n",
        "y_train = y_train[indices_train]\n",
        "\n",
        "x_train = x_train.to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#############################\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LX2hkRe1cdmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f46158-b2ce-4a9b-c4d1-e6cc98dcb3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2983, 784) (2983,)\n"
          ]
        }
      ],
      "source": [
        "# Modify `y_test` and `x_test`.\n",
        "# Only keep the 3 classes mentioned above.\n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "\n",
        "y_test = y_test.to_numpy()\n",
        "\n",
        "# Get the indices of the samples belonging to the above classes in the testing set\n",
        "indices_test = np.where(np.isin(y_test, classes))[0]\n",
        "\n",
        "# Keep only the samples of the above classes in the testing set\n",
        "x_test = x_test.iloc[indices_test]\n",
        "y_test = y_test[indices_test]\n",
        "x_test = x_test.to_numpy()\n",
        "#############################\n",
        "\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv6SMLUktWbv"
      },
      "source": [
        "## Linear & Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXlyJo5JteKC"
      },
      "source": [
        "In this part, you'll implement the forward and backward process for the following components:\n",
        "- Softmax Layer\n",
        "- Linear Layer\n",
        "- ReLU Layer\n",
        "- Sigmoid Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXtAD5uYA4sQ"
      },
      "source": [
        "### The `Softmax` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tzaIVo-_Axp7"
      },
      "outputs": [],
      "source": [
        "class SoftMaxLayer(object):\n",
        "    def __init__(self):\n",
        "        self.inp = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Write the forward pass for softmax.\n",
        "        # Save the values required for the backward pass.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.inp = x\n",
        "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        #############################\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, up_grad):\n",
        "        # Write the backward pass for softmax.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        grad = self.output * (up_grad - np.sum(self.output * up_grad, axis=1, keepdims=True))\n",
        "        return grad\n",
        "        #############################\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcFoIDZjcdnB"
      },
      "source": [
        "### The `Linear` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1strsTh6cdnG"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        # Initialize the layer's weights and biases\n",
        "        #############################\n",
        "        # Your code goes here (2 points)\n",
        "\n",
        "        # self.w = np.random.randn(out_dim, in_dim) * 0.01\n",
        "        # self.b = np.zeros((out_dim, 1))\n",
        "\n",
        "\n",
        "        self.w = np.random.randn(in_dim, out_dim) * np.sqrt(2. / in_dim)\n",
        "        self.b = np.zeros((1, out_dim))\n",
        "        self.inp = None\n",
        "\n",
        "        #############################\n",
        "        self.dw = None\n",
        "        self.db = None\n",
        "        self.dx = None\n",
        "\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Compute linear layer's output.\n",
        "        # Save the value(s) required for the backward phase.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.inp = inp\n",
        "        z = np.dot(inp, self.w) + self.b\n",
        "        # z = self.w @ inp + self.b\n",
        "        #############################\n",
        "\n",
        "        return z\n",
        "\n",
        "    def backward(self, up_grad):\n",
        "        # Calculate the gradient with respect to the weights\n",
        "        # and biases and save the results.\n",
        "        #############################\n",
        "        # Your code goes here (6 points)\n",
        "        inp_2d = np.atleast_2d(self.inp)\n",
        "        self.dw = np.dot(inp_2d.T, up_grad)\n",
        "        self.db = np.sum(up_grad, axis=0, keepdims=True)\n",
        "        down_grad = np.dot(up_grad, self.w.T)\n",
        "        #############################\n",
        "        return down_grad\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      # Update the layer's weights and biases\n",
        "      # Update previous_w_update and previous_b_update accordingly\n",
        "      #############################\n",
        "      # Your code goes here (5 points)\n",
        "      self.w  = optimizer.get_next_update(self.w, self.dw)\n",
        "      self.b = optimizer.get_next_update(self.b, self.db)\n",
        "      #############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0Lfo-nhcdnG"
      },
      "source": [
        "### The `ReLU` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tN6vcirMcdnH"
      },
      "outputs": [],
      "source": [
        "class RelU:\n",
        "    def __init__(self):\n",
        "        self.inp = None\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Write the forward pass for ReLU.\n",
        "        # Save the value(s) required for the backward pass.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.inp = inp\n",
        "        output = np.maximum(0, inp)\n",
        "        #############################\n",
        "        return output\n",
        "\n",
        "    def backward(self, up_grad):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        down_grad = up_grad * (self.inp > 0)\n",
        "        return down_grad\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z00KoSI3cdnJ"
      },
      "source": [
        "### The `sigmoid` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TTYYeL2lcdnJ"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, inp):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.out = 1 / (1 + np.exp(-inp))\n",
        "        #############################\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, up_grad):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        down_grad = up_grad * self.out * (1 - self.out)\n",
        "        #############################\n",
        "        return down_grad\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zngleGY2cdnK"
      },
      "source": [
        "## `Loss` function :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISedT4FvcdnK"
      },
      "source": [
        "For this task we are going to use the [Cross-Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XQyz4ybycdnL"
      },
      "outputs": [],
      "source": [
        "class CELoss():\n",
        "    def __init__(self):\n",
        "      self.yhat = None\n",
        "      self.y = None\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        m = pred.shape[0]\n",
        "        self.yhat = pred\n",
        "        self.y = target\n",
        "\n",
        "        # Commpute and return the loss\n",
        "        #############################\n",
        "        # Your code goes here (8 points)\n",
        "        loss = -1/m * np.sum(target * np.log(pred))\n",
        "        return loss\n",
        "        #############################\n",
        "\n",
        "\n",
        "    def backward(self):\n",
        "        # Derivative of loss_fn with respect to a the predicted label.\n",
        "        # Use `self.y` and `self.yhat` to compute and return `grad`.\n",
        "        #############################\n",
        "        # Your code goes here (6 points)\n",
        "        m = self.y.shape[0]\n",
        "        grad = -1/m * (self.y / self.yhat)\n",
        "        #############################\n",
        "        return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xovZI-70kB9I"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "In this section, you'll implement an optimizer classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "h5ADTi5tkVTS"
      },
      "outputs": [],
      "source": [
        "class GradientDescent(object):\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def get_next_update(self, x, dx):\n",
        "        # Compute the new value for 'x' and return the result\n",
        "        #############################\n",
        "        # Your code goes here (2 points)\n",
        "        x = x - self.lr * dx\n",
        "        return x\n",
        "        #############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxxrEEovYEFi"
      },
      "source": [
        "## The Model\n",
        "Now you'll write the base class for a multi-layer perceptron network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t8SoZeYRcdnY"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, layers, loss_fn, optimizer):\n",
        "        self.layers = layers\n",
        "        self.losses  = []\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Pass `inp` to all the layers sequentially\n",
        "        # and return the result.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        out = inp\n",
        "        for layer in self.layers:\n",
        "          out = layer.forward(out)\n",
        "\n",
        "        return out\n",
        "        #############################\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        loss = self.loss_fn.forward(pred, label)\n",
        "        self.losses.append(loss)\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        # Start with loss function's gradient and\n",
        "        # do the backward pass on all the layers.\n",
        "        #############################\n",
        "        # Your code goes here (5 points)\n",
        "        grad = self.loss_fn.backward()\n",
        "        for layer in reversed(self.layers):\n",
        "          grad = layer.backward(grad)\n",
        "        #############################\n",
        "\n",
        "    def update(self):\n",
        "        for layer in self.layers:\n",
        "          layer.step(self.optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo0rNwYciueF"
      },
      "source": [
        "The following cell encodes training labels into a one-hot representation with 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nhJTulaFJ4vR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea187a1-0fea-427a-8e11-c1bfddc4e7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18017, 3)\n",
            "(2983, 3)\n"
          ]
        }
      ],
      "source": [
        "def onehot_enc(y, num_labels):\n",
        "    y = np.array(y).astype(int)\n",
        "    ary = np.zeros((y.shape[0], num_labels))\n",
        "    print(ary.shape)\n",
        "    for i, val in enumerate(y):\n",
        "        if val == 7:\n",
        "          ary[i, 2] = 1\n",
        "        else:\n",
        "          ary[i, val] = 1\n",
        "    return ary\n",
        "\n",
        "y_train = onehot_enc(y_train, 3)\n",
        "y_test = onehot_enc(y_test, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "TS6S_RUwsRkF"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, x_train, y_train):\n",
        "    for n in range(epochs):\n",
        "      # First do the forward pass. Next, compute the loss.\n",
        "      # Then do the backward pass and finally, update the parameters.\n",
        "      #############################\n",
        "      # Your code goes here (4 points)\n",
        "\n",
        "      pred = model.forward(x_train) # forward pass\n",
        "      loss = model.loss(pred, y_train) # compute loss\n",
        "      model.backward() # backward pass\n",
        "      model.update() # update parameters\n",
        "      #############################\n",
        "      print(f\"Loss at {n}: {loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "m1lSq2jNcdnY",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f6d10d-75be-4173-93d7-adabed99aacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at 0: 1.136\n",
            "Loss at 1: 0.986\n",
            "Loss at 2: 0.861\n",
            "Loss at 3: 0.805\n",
            "Loss at 4: 0.771\n",
            "Loss at 5: 0.746\n",
            "Loss at 6: 0.726\n",
            "Loss at 7: 0.709\n",
            "Loss at 8: 0.695\n",
            "Loss at 9: 0.683\n",
            "Loss at 10: 0.672\n",
            "Loss at 11: 0.664\n",
            "Loss at 12: 0.656\n",
            "Loss at 13: 0.650\n",
            "Loss at 14: 0.644\n",
            "Loss at 15: 0.640\n",
            "Loss at 16: 0.635\n",
            "Loss at 17: 0.631\n",
            "Loss at 18: 0.628\n",
            "Loss at 19: 0.625\n",
            "Loss at 20: 0.622\n",
            "Loss at 21: 0.620\n",
            "Loss at 22: 0.617\n",
            "Loss at 23: 0.615\n",
            "Loss at 24: 0.613\n",
            "Loss at 25: 0.612\n",
            "Loss at 26: 0.610\n",
            "Loss at 27: 0.608\n",
            "Loss at 28: 0.607\n",
            "Loss at 29: 0.606\n",
            "Loss at 30: 0.604\n",
            "Loss at 31: 0.603\n",
            "Loss at 32: 0.602\n",
            "Loss at 33: 0.601\n",
            "Loss at 34: 0.600\n",
            "Loss at 35: 0.599\n",
            "Loss at 36: 0.598\n",
            "Loss at 37: 0.598\n",
            "Loss at 38: 0.597\n",
            "Loss at 39: 0.596\n",
            "Loss at 40: 0.595\n",
            "Loss at 41: 0.595\n",
            "Loss at 42: 0.594\n",
            "Loss at 43: 0.594\n",
            "Loss at 44: 0.593\n",
            "Loss at 45: 0.592\n",
            "Loss at 46: 0.592\n",
            "Loss at 47: 0.591\n",
            "Loss at 48: 0.591\n",
            "Loss at 49: 0.590\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the `MLP` with the following structure:\n",
        "#     Linear with 50 units --> ReLU --> Linear with 50 units --> ReLU --> Linear with 3 units --> Sigmoid --> Softmax\n",
        "# Use GradientDescent as the optimizer, set the learning rate to 0.001, and use CELoss as the loss function.\n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "nn = MLP(\n",
        "    layers=[\n",
        "        Linear(784, 50), # Input dimension is 784, output dimension is 50\n",
        "        RelU(), # Activation function for the first hidden layer\n",
        "        Linear(50, 50), # Input and output dimension are both 50\n",
        "        RelU(), # Activation function for the second hidden layer\n",
        "        Linear(50, 3), # Input dimension is 50, output dimension is 3\n",
        "        Sigmoid(), # Activation function for the output layer\n",
        "        SoftMaxLayer() # Softmax layer for the final output\n",
        "    ],\n",
        "    loss_fn=CELoss(), # Use cross-entropy loss as the loss function\n",
        "    optimizer=GradientDescent(lr=0.1) # Use gradient descent as the optimizer with learning rate 0.001\n",
        ")\n",
        "#############################\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "# Train the network using only `x_train` and `y_train` (no validation)\n",
        "train(nn, epochs, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJec2xRJmY37"
      },
      "source": [
        "Let's plot the loss value for each iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ymaQNn70cdnZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "9e3e1e6b-a91f-4d56-dc69-0eb5a2b7aadd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(nn.losses)\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz3yqRa1cdna"
      },
      "source": [
        "**Let's also check our model's performance using the `accuracy` metric on the `testing` dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "TRqwXho7cdnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f068ce-e66c-456b-ff36-226270cf1cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9792155548105934\n"
          ]
        }
      ],
      "source": [
        "# Compute the accuracy on the testing set\n",
        "#############################\n",
        "# Your code goes here (7 points)\n",
        "\n",
        "# Get the predictions on the testing set\n",
        "pred = nn.forward(x_test)\n",
        "# Compare the predictions with the true labels and count the correct matches\n",
        "correct = np.sum(pred.argmax(1) == y_test.argmax(1))\n",
        "\n",
        "# Calculate the accuracy score\n",
        "acc = correct / x_test.shape[0]\n",
        "\n",
        "#############################\n",
        "\n",
        "print(acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}